########################################################################################################
# align_snake
#   Snakemake workflow to  download fastq files and associated metadata from SRA, then use STARsolo to align and quantify 10x Chromium datasets, and finally perform simple preprocessing so that data are ready for downstream analysis
#   Written by David W. McKellar
########################################################################################################

import pandas as pd
from itertools import chain
from functools import reduce
from os import path, remove, rename

########################################################################################################
# Config file
########################################################################################################
configfile:'config.yaml'

########################################################################################################
# Directories and locations
########################################################################################################
PRODIR = config['PRODIR']
METADIR = config['METADIR']
TMPDIR = config['TMPDIR'] # temporary files
DATADIR = config['DATADIR'] # outputs for STAR alignment, etc
REFDIR = config['REFDIR'] # Output for reference genomes

shell("mkdir -p {PRODIR}")
shell("mkdir -p {TMPDIR}")
shell("mkdir -p {METADIR}")
shell("mkdir -p {DATADIR}")
shell("mkdir -p {REFDIR}")

########################################################################################################
# Metadata
########################################################################################################
META = pd.read_csv(config['SAMPLE_SHEET'], na_filter=False)

META = META[list(META['include'])] #remove undesired SRRs ('include'==False)
META = META[list(META['file.format']!="ERR")] #remove SRRs that didn't download properly [YET!]

META = META[list(META['tissue']=="muscle")]
META = META[list(META['species']=="Homo sapiens")]

META = META[list(META['GSM.accession']!="NA")]
META = META[list(META['GSM.accession']!="")]
META = META.iloc[1:13,:] # subset to only download certain SRRs...

# Get list of SRR IDs for fastq downloading
SRR_LIST = list(META['SRR.accession'])
for i in range(0,len(SRR_LIST)):
    SRR_LIST[i] = SRR_LIST[i].split(";")
SRR_LIST = list(chain(*SRR_LIST))

# Grab SRR IDs and their associated fastqs
SRRS = META['GSM.accession'].unique()

# Build dictionary of SRR IDs/fastq files
R1_FQs = {}
R2_FQs = {}
for GSM in SRRS:
    TMP_SRR = META['SRR.accession'][list(META["GSM.accession"]==GSM)].values[0]
    TMP_SRR = TMP_SRR.split(";")
    R1_FQs[GSM] = [DATADIR+"/fastqs/"+ID+"_R1.fastq.gz" for ID in TMP_SRR]
    R2_FQs[GSM] = [DATADIR+"/fastqs/"+ID+"_R2.fastq.gz" for ID in TMP_SRR]

########################################################################################################
# Reference genome info
########################################################################################################
SPECIES = list(META['species']) #all lowercase and underscores (no spaces!)
SPECIES = [x.lower() for x in SPECIES]
SPECIES = [x.replace(' ', '_') for x in SPECIES]
SPECIES = pd.unique(SPECIES)

########################################################################################################
# Executables
########################################################################################################
FFQ_EXEC = config["FFQ_EXEC"]
PREFETCH_EXEC = config["PREFETCH_EXEC"]
PFD_EXEC = config["PFD_EXEC"]
BAM2FQ_EXEC = config["BAM2FQ_EXEC"]

STAR_EXEC = config['STAR_EXEC']
GGET_EXEC = config['GGET_EXEC']
FASTQC_EXEC = config['FASTQC_EXEC']
# TRIMGALORE_EXEC = config['TRIMGALORE_EXEC']
CUTADAPT_EXEC = config['CUTADAPT_EXEC']
#TODO- samtools, etc

########################################################################################################
rule all:
    input:
        expand('{DATADIR}/fastqs/{SRR}_{READ}.fastq.gz', DATADIR=config['DATADIR'], SRR=SRR_LIST, READ=["R1", "R2"]), # R1 fastq
        # expand('{TMPDIR}/{SRR}.txt', TMPDIR=config['TMPDIR'], SRR=SRR), # **Only use for debugging**
        expand('{METADIR}/{SRR}.json', METADIR=config['METADIR'], SRR=SRR_LIST), # metadata files
        expand('{METADIR}/merged_metadata.csv', METADIR=config['METADIR']),

        expand('{REFDIR}/{SPECIES}/STAR/SAindex', REFDIR=REFDIR, SPECIES=SPECIES), # Reference genomes
        # expand('{DATADIR}/{SRR}/Solo.out/Gene/filtered/matrix.mtx', DATADIR=config['DATADIR'], SRR=SRRS),
        # expand('{DATADIR}/{SRR}/Aligned.sortedByCoord.dedup.out.bam.bai', DATADIR=config['DATADIR'], SRR=SRRS), # umi_tools deduplicated .bam **Note** this is super slow!! Only uncomment if NEEDED
        # expand('{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam.bai', DATADIR=config['DATADIR'], SRR=SRRS), #non-deduplicated .bam; used for saturation estimation
        expand('{DATADIR}/{SRR}/preTrim_fastqc_R2_out', DATADIR=config['DATADIR'], SRR=SRRS), # raw R2 fastQC results
        expand('{DATADIR}/{SRR}/postTrim_fastqc_R2_out', DATADIR=config['DATADIR'], SRR=SRRS) # adapter/polyA/ployG-trimmed R2 fastQC results
        # expand('{DATADIR}/{SRR}/qualimap_out/qualimapReport.html', DATADIR=config['DATADIR'], SRR=SRRS), # alignment QC qith qualimap plotgardener)

        # expand('{DATADIR}/{SRR}/Unmapped_fastqc_out', DATADIR=config['DATADIR'], SRR=SRRS), #fastQC results for unmapped reads
        # expand('{DATADIR}/{SRR}/Solo.out/Gene/filtered/matrix.mtx.gz', DATADIR=config['DATADIR'], SRR=SRRS), # count mats


#############################################
## Get metadata with `ffq`
#############################################
# Download metadata for each individual SRR (run) ID
## Info: https://github.com/pachterlab/ffq
rule get_metadata:
    output:
        METAJSON = '{METADIR}/{SRR}.json'
    shell:
        """
        {FFQ_EXEC} -o {output.METAJSON} {wildcards.SRR}
        """
        # sleep 3

# Combine ffq-downloaded .json's into a big csv for parsing
rule merge_metadata:
    input:
        expand('{METADIR}/{SRR}.json', METADIR=config['METADIR'], SRR=SRR_LIST)
    output:
        METACSV = '{METADIR}/merged_metadata.csv'
    run:
        df_list = list()
        for f in input: # load metadata files for each SRR
            with open(f) as data_file:
                data = json.load(data_file)
            df = pd.json_normalize(data).T
            tmp = df.index[0].split('.')[0] + '.' # Get SRR ID...
            df = df.rename(index = lambda x: x.strip(tmp)) # Remove SRR ID from row names
            df_list.append(df.T) # transpose so that each row is a different run

        print("Concatenating metadata...")
        df_out = pd.concat(df_list)

        df_out.to_csv(output.METACSV, index=False)

#############################################
## prefetch (SRA-toolkit)
#############################################
# Download reads as an SRA file into a temporary directory, so they can be formatted as .fastq's
# Data uploaded as a .bam is downloaded in the original format (.bam file) which will later be converted to .fastq
rule prefetch:
    output:
        SRA_check = temp("{TMPDIR}/{SRR}.txt")
    params:
        # TMPDIR=TMPDIR,
        # SRA_tmp = "{TMPDIR}/{SRR}.sra",
        SRA_BAM = '{TMPDIR}/{SRR}.bam',
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in srr_id for srr_id in META['SRR.accession']],"file.format"].values[0]
    run:
        if params.UPLOADTYPE == "fastq":
            shell(
                """
                echo "Prefetching SRA files for {wildcards.SRR}"

                {PREFETCH_EXEC} \
                --verify yes \
                --max-size 999999999999 \
                --output-directory {TMPDIR} \
                {wildcards.SRR}

                if test -f "{TMPDIR}/{wildcards.SRR}.sra"; then
                    echo fastq > {output.SRA_check}
                fi

                """
            )
        elif params.UPLOADTYPE == "bam":
            #TODO- check for/remove extra files from SRA
            #*Note- `--type all` downloads the original uploaded file(s); very important that the .bam is in its original format and that it was aligned w/ cellranger
            shell(
            #Downloaded bams are not readable....
                """
                cd {TMPDIR}

                {PREFETCH_EXEC} \
                --verify yes \
                --max-size 999999999999 \
                --force ALL \
                --output-directory {TMPDIR}/{wildcards.SRR} \
                --type all \
                {wildcards.SRR}

                mv $(ls -a {TMPDIR}/{wildcards.SRR}/*.bam) {TMPDIR}/{wildcards.SRR}.bam
                echo bam > {output.SRA_check}
                """
            )
            # --output-file {SRA_BAM} \
            #find {TMPDIR}/{wildcards.SRR} -name

#############################################
## parallel-fastq-dump
##      https://github.com/rvalieris/parallel-fastq-dump
#############################################
 # Convert SRA files to fastq with parallel-fastq-dump
rule get_fastqs:
    input:
        SRA_check = TMPDIR+"/{SRR}.txt"
    output:
        R1_FQ = '{DATADIR}/fastqs/{SRR}_1.fastq.gz',
        R2_FQ = '{DATADIR}/fastqs/{SRR}_2.fastq.gz'
    params:
        # TMPDIR = TMPDIR,
        SRA_tmp = TMPDIR+'/{SRR}.sra',
        SRA_BAM = TMPDIR+'/{SRR}.bam',
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in srr_id for srr_id in META['SRR.accession']],"file.format"].values[0],
        # UPLOADTYPE="bam", #for debugging
        OUTDIR = '{DATADIR}/fastqs/'
    threads:
        config["CORES_LO"]
    run:
        if params.UPLOADTYPE == "fastq":
            shell(
                """
                echo "Converting SRA files to .fastq's for {wildcards.SRR}"
                {PFD_EXEC} \
                --sra-id {params.SRA_tmp} \
                --threads {threads} \
                --outdir {params.OUTDIR} \
                --tmpdir {TMPDIR} \
                --split-files \
                --gzip && \
                rm {params.SRA_tmp}
                """
            )
        elif params.UPLOADTYPE == "bam":
            #TODO- remove extra files in {TMPDIR}/{wildcards.SRR}/...
            shell(
                """
                rm -rf {TMPDIR}/{wildcards.SRR}/b2f/

                {BAM2FQ_EXEC} \
                --nthreads {threads} \
                --reads-per-fastq=999999999999999 \
                {params.SRA_BAM} \
                {TMPDIR}/{wildcards.SRR}/b2f/

                zcat $(find {TMPDIR}/{wildcards.SRR}/b2f/ -name *R1*.fastq.gz) > {params.OUTDIR}/{wildcards.SRR}_1.fastq
                zcat $(find {TMPDIR}/{wildcards.SRR}/b2f/ -name *R2*.fastq.gz) > {params.OUTDIR}/{wildcards.SRR}_2.fastq

                pigz -p {threads} {params.OUTDIR}/{wildcards.SRR}_*.fastq

                rm -r {TMPDIR}/{wildcards.SRR}/b2f/
                """
            )

# {FFQ} --ncbi {SRR} \
# | xargs curl -o {DATADIR}/fastqs/{SRR}.bam

# Other option:
# ffq --ftp SRR7276476 | grep -Eo '"url": "[^"]*"' | grep -o '"[^"]*"$' | xargs curl -O

# Check fastq sizes, then rename/label files for R1 & R2 (discard I1)
#TODO- account for dual index (v3.1) SRRs!!
rule rename_fastqs:
    input:
        R1_FQ = '{DATADIR}/fastqs/{SRR}_1.fastq.gz',
        R2_FQ = '{DATADIR}/fastqs/{SRR}_2.fastq.gz'
    output:
        R1_FQ = '{DATADIR}/fastqs/{SRR}_R1.fastq.gz',
        R2_FQ = '{DATADIR}/fastqs/{SRR}_R2.fastq.gz'
    params:
        OUTDIR = '{DATADIR}/fastqs/',
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in srr_id for srr_id in META['SRR.accession']],"file.format"].values[0]
    run:
        #TODO- rewrite this into a function to clean up snakemake code...
        if params.UPLOADTYPE == "fastq":
            # Check for file sizes (R2 is biggest, then R1, then I2 & I1)
            if path.exists(f'{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz'):
                sra_sizes = {
                    f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz')
                }
                rename_dict = {
                    'R2': max(sra_sizes),
                    'R1': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-3])],
                    'I2': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                    'I1': min(sra_sizes)
                }

                remove(rename_dict['I2'])
                remove(rename_dict['I1'])
                rename(rename_dict['R1'], f'{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz')
                rename(rename_dict['R2'], f'{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz')
            elif path.exists(f'{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz'):
                sra_sizes = {
                    f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz')
                }
                rename_dict = {
                    'R2': max(sra_sizes),
                    'R1': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                    'I1': min(sra_sizes)
                }

                remove(rename_dict['I1'])
                rename(rename_dict['R1'], f'{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz')
                rename(rename_dict['R2'], f'{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz')
            else:
                sra_sizes = {
                    f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz'),
                    f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz': path.getsize(f'{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz')
                }

                rename_dict = {
                    'R2': max(sra_sizes),
                    'R1': min(sra_sizes)
                }

                rename(rename_dict['R1'], f'{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz')
                rename(rename_dict['R2'], f'{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz')
        elif params.UPLOADTYPE == "bam":
            #rename R1 & R2 files (can use R1 and R2 from file names here, not dependent on file sizes)
            shell(
                """
                mv {input.R1_FQ} {output.R1_FQ}
                mv {input.R2_FQ} {output.R2_FQ}
                """
            )


#############################################
## Generate references w/ `gget`
#############################################
# Unzip the whitelist file if it hasn't been done yet
#TODO: change to download from somehwere? maybe look for a wget link?
#TODO: include this in `resources/chemistry_sheet.csv`
# rule gunzipWhitelists:
#     output:
#         expand("{PRODIR}/resources/whitelists/{CHEM_WHITE}", CHEM_WHITE =unique(CHEM_SHEET["CB_WHITELIST"])
#     shell:
#         """
#         #DOwnload & gunzip cb whitleist(s)
#         """

# Build reference genomes for STARsolo alignment/quantification
#TODO: parallelize across species!
rule build_refs:
    output:
        REF_METADATA = expand("{REFDIR}/{SPECIES}/STAR/metadata.json", REFDIR=config['REFDIR'], SPECIES=SPECIES),
        REF = expand("{REFDIR}/{SPECIES}/STAR/SAindex", REFDIR=config['REFDIR'], SPECIES=SPECIES) # Reference genomes
    threads:
        config['CORES_HI']
    run:
        for S in SPECIES:
            print(f"Downloading genome sequence and annotations for {S} to {REFDIR}/{S}")
            shell(
                f"""
                mkdir -p {REFDIR}/{S}
                cd {REFDIR}/{S}

                {GGET_EXEC} ref \
                --out {REFDIR}/{S}/metadata.json \
                --which gtf,dna \
                --download \
                {S}

                gunzip {REFDIR}/{S}/*.gz
                """
            )

            # Build reference for STAR
            print(f"Building STAR reference for {S}...\n")
            shell(
                f"""
                {STAR_EXEC} \
                --runThreadN {threads} \
                --runMode genomeGenerate \
                --genomeDir {REFDIR}/{S}/STAR \
                --genomeFastaFiles $(ls -t {REFDIR}/{S}/*.fa) \
                --sjdbGTFfile $(ls -t {REFDIR}/{S}/*.gtf) \
                --sjdbGTFfeatureExon exon

                pigz -p {threads} $(ls -t {REFDIR}/{S}/*.fa)
                pigz -p {threads} $(ls -t {REFDIR}/{S}/*.gtf)
                """
            )

#############################################
## Trimming and FastQC
#############################################

# Merge .fastq files (in case more than one sesquencing run was performed)
rule merge_fastqs:
    output:
        MERGED_R1_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R1.fq.gz'),
        MERGED_R2_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R2.fq.gz')
    params:
        TMP_DIR = '{DATADIR}/{SRR}/tmp',
        R1_FQ = lambda wildcards: R1_FQs[wildcards.SRR],
        R2_FQ = lambda wildcards: R2_FQs[wildcards.SRR]
    threads:
        config['CORES_LO']
    run:
        if len(params.R1_FQ)==1 & len(params.R2_FQ)==1: # shell for single fastq input
            shell("cp {params.R1_FQ} {output.MERGED_R1_FQ}")
            shell("cp {params.R2_FQ} {output.MERGED_R2_FQ}")
        else: # shell enablinging multi-fast input; concatenate inputs
            print("Concatenating",len(params.R1_FQ), ".fastq's for", wildcards.SRR)
            shell("mkdir -p {params.TMP_DIR}")
            shell("zcat {params.R1_FQ} > {params.TMP_DIR}/{wildcards.SRR}_R1.fq")
            shell("zcat {params.R2_FQ} > {params.TMP_DIR}/{wildcards.SRR}_R2.fq")
            shell("pigz -p {threads} {params.TMP_DIR}/*.fq")

# Check QC of reads before trimming
rule preTrim_FastQC_R2:
    input:
        MERGED_R2_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R2.fq.gz'
    output:
        fastqcDir = directory('{DATADIR}/{SRR}/preTrim_fastqc_R2_out'),
        # fastqcReport = ''
    threads:
        config['CORES_LO']
        # min([config['CORES_LO'],8]) # 8 core max based on recommendations from trim_galore authors
    shell:
        """
        mkdir -p {output.fastqcDir}
        cd {output.fastqcDir}

        fastqc \
        --outdir {output.fastqcDir} \
        --threads {threads} \
        {input.MERGED_R2_FQ}
        """

# TSO & polyA trimming
rule trimPolyA_R2:
    input:
        MERGED_R1_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R1.fq.gz',
        MERGED_R2_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R2.fq.gz'
    output:
        A_TRIMMED_R1_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R1_Atrimmed.fq.gz'),
        A_TRIMMED_R2_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R2_Atrimmed.fq.gz'),
        POLYA_REPORT = '{DATADIR}/{SRR}/cutadapt_polyA_report.txt'
    params:
        CUTADAPT_EXEC = CUTADAPT_EXEC,
        THREE_PRIME_R2_POLYA = "A"*100,
        FIVE_PRIME_R2 = "CCCATGTACTCTGCGTTGATACCACTGCTT" #10x TSO sequence
        # FIVE_PRIME_R2 = "TTCGTCACCATAGTTGCGTCTCATGTACCC" #rev 10x TSO sequence
    threads:
        config['CORES_LO']
        # min([config['CORES_LO'],8]) # 8 core max based on recommendations from trim_galore authors
    log:
        '{DATADIR}/{SRR}/cutadapt_polyA_report.txt'
    shell:
        """
        {params.CUTADAPT_EXEC} \
        --minimum-length 18 \
        -A {params.THREE_PRIME_R2_POLYA} \
 		-G {params.FIVE_PRIME_R2} \
        --pair-filter=any \
 		-o {output.A_TRIMMED_R1_FQ} \
        -p {output.A_TRIMMED_R2_FQ} \
        --cores {threads} \
        {input.MERGED_R1_FQ} {input.MERGED_R2_FQ} 1> {log}
        """

#Additional trimming step, for 2-color Illumina chemistries (NextSeq, etc.)
rule trimPolyG_R2:
    input:
        A_TRIMMED_R1_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R1_Atrimmed.fq.gz',
        A_TRIMMED_R2_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R2_Atrimmed.fq.gz'
    output:
        FINAL_R1_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R1_final.fq.gz'),
        FINAL_R2_FQ = temp('{DATADIR}/{SRR}/tmp/{SRR}_R2_final.fq.gz'),
        POLYG_REPORT = '{DATADIR}/{SRR}/cutadapt_polyG_report.txt'
    params:
        CUTADAPT_EXEC = CUTADAPT_EXEC,
        THREE_PRIME_R2_POLYG = "G"*100,
        FIVE_PRIME_R2 = "AAGCAGTGGTATCAACGCAGAGTACATGGG" # rev-comp of 10x TSO sequence
    threads:
        config['CORES_LO']
        # min([config['CORES_LO'],8]) # 8 core max based on recommendations from trim_galore authors
    log:
        log = '{DATADIR}/{SRR}/cutadapt_polyG_report.txt'
    shell:
        """
        {params.CUTADAPT_EXEC} \
        --minimum-length 18 \
        -A {params.THREE_PRIME_R2_POLYG} \
 		-G {params.FIVE_PRIME_R2} \
        --pair-filter=any \
 		-o {output.FINAL_R1_FQ} \
        -p {output.FINAL_R2_FQ} \
        --cores {threads} \
        {input.A_TRIMMED_R1_FQ} {input.A_TRIMMED_R2_FQ} 1> {log.log}
        """

rule postTrim_FastQC_R2:
    input:
        FINAL_R2_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R2_final.fq.gz'
    output:
        fastqcDir = directory('{DATADIR}/{SRR}/postTrim_fastqc_R2_out')
        # fastqcReport = ''
    threads:
        min([config['CORES_LO'],8]) # 8 core max
    shell:
        """
        mkdir -p {output.fastqcDir}
        cd {output.fastqcDir}

        fastqc \
        --outdir {output.fastqcDir} \
        --threads {threads} \
        {input.FINAL_R2_FQ}
        """

#############################################
## Alignment
#############################################
# Make output directory, align fastqs, and generate raw/filtered feature/cell-barcode matrices
#   Info for STARsolo command line paramaters: https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md

rule STARsolo_align:
    input:
        CB_WHITELIST = config['CB_WHITELIST'],#TODO- make chemistry-specific
        FINAL_R1_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R1_final.fq.gz',
        FINAL_R2_FQ = '{DATADIR}/{SRR}/tmp/{SRR}_R2_final.fq.gz',
        REF_LIST = expand('{REFDIR}/{SPECIES}/STAR_REF_FILE_NAME', REFDIR=REFDIR, SPECIES=SPECIES) # Reference genomes
    output:
        SORTEDBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam', #TODO: add temp()
        UNMAPPED1 = '{DATADIR}/{SRR}/Unmapped.out.mate1',
        UNMAPPED2 = '{DATADIR}/{SRR}/Unmapped.out.mate2',
        VELMAT = '{DATADIR}/{SRR}/Solo.out/Velocyto/raw/spliced.mtx.gz',
        GENEMAT = '{DATADIR}/{SRR}/Solo.out/Gene/raw/matrix.mtx.gz',
        GENEFULLMAT = '{DATADIR}/{SRR}/Solo.out/GeneFull/raw/matrix.mtx.gz'
    params:
        CHEMISTRY = lambda wildcards: \
            META.loc[[wildcards.SRR in srr_id for srr_id in META['SRR.accession']],'file.format'].values[0],
        DATADIR = config['DATADIR'],
        STAR_EXEC = config['STAR_EXEC'],
        # STAR_REF = config['STAR_REF'],#TODO-species-specific!
        UMIlen = config['UMIlen'], # pull from chemistry_sheet
        MEMLIMIT = config['MEMLIMIT']
    threads:
        config['CORES_LO']
    shell:
        """
        mkdir -p {params.DATADIR}/{wildcards.SRR}

        {params.STAR_EXEC} \
        --runThreadN {threads} \
        --outFileNamePrefix {params.DATADIR}/{wildcards.SRR}/ \
        --outSAMtype BAM SortedByCoordinate \
        --outSAMattributes NH HI nM AS CR UR CB UB GX GN sS sQ sM \
        --readFilesCommand zcat \
        --soloUMIlen {params.UMIlen} \
        --genomeDir {params.REF_LIST} \
        --genomeLoad LoadAndKeep \
        --limitBAMsortRAM={params.MEMLIMIT} \
        --readFilesIn {input.FINAL_R2_FQ} {input.FINAL_R1_FQ} \
        --clipAdapterType CellRanger4 \
        --outReadsUnmapped Fastx \
        --outFilterMultimapNmax 50 \
        --soloType CB_UMI_Simple \
        --soloCBwhitelist {input.CB_WHITELIST} \
        --soloCellFilter EmptyDrops_CR \
        --soloFeatures Gene GeneFull Velocyto \
        --soloMultiMappers EM
        """

        # compress outputs
        # shell(
        #     """
        #     gzip -qf {output.GENE}/raw/*
        #     gzip -qf {output.GENE}/filtered/*
        #
        #     gzip -qf {output.GENEFULL}/raw/*
        #     gzip -qf {output.GENEFULL}/filtered/*
        #
        #     gzip -qf {output.VEL}/raw/*
        #     gzip -qf {output.VEL}/filtered/*
        #     """
        # )

# Index the .bam produced by STAR
rule indexSortedBAM:
    input:
        SORTEDBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam'
    output:
        BAI = '{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam.bai'
    threads:
        config['CORES_LO']
    shell:
        """
        samtools index -@ {threads} {input.SORTEDBAM}
        """


# Remove reads that don't have a corrected spot/cell barcode with samtools, then remove duplicates w/ **umi-tools**
## High mem usage? Check here! https://umi-tools.readthedocs.io/en/latest/faq.html
## **WARNING** this step is suuuupppppeeerrrr sloowwww. Don't run it if you don't need to!
rule umitools_dedupBAM:
    input:
        CB_WHITELIST = config['CB_WHITELIST'],
        SORTEDBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam'
    output:
        DEDUPBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.dedup.out.bam',
        TMPBAM = temp('{DATADIR}/{SRR}/tmp.bam')
    params:
        OUTPUT_PREFIX='{DATADIR}/{SRR}/umitools_dedup/{SRR}',
        # TMPBAM = '{DATADIR}/{SRR}/tmp.bam'
    threads:
        config['CORES_LO']
        #1
    log:
        '{DATADIR}/{SRR}/umitools_dedup/dedup.log'
    shell:
        """
        samtools view -1 -b \
        -@ {threads} \
        --tag-file CB:{input.CB_WHITELIST} \
        {input.SORTEDBAM} \
        > {output.TMPBAM}

        samtools index \
        -@ {threads} \
        {output.TMPBAM}

        umi_tools dedup \
        -I {output.TMPBAM} \
        --extract-umi-method=tag \
        --umi-tag=UB \
        --cell-tag=CB \
        --method=unique \
        --per-cell \
        --unmapped-reads=discard \
        --output-stats={params.OUTPUT_PREFIX} \
        --log {log} \
        -S {output.DEDUPBAM}
        """

rule umitools_indexDedupBAM:
    input:
        SORTEDBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.dedup.out.bam'
    output:
        BAI = '{DATADIR}/{SRR}/Aligned.sortedByCoord.dedup.out.bam.bai'
    threads:
        config['CORES_LO']
    shell:
        """
        samtools index -@ {threads} {input.SORTEDBAM}
        """

#############################################
## QC on STAR outputs
#############################################

## qualimap on aligned reads
rule qualimapQC:
    input:
        SORTEDBAM = '{DATADIR}/{SRR}/Aligned.sortedByCoord.out.bam'
    output:
        qualimapDir = directory('{DATADIR}/{SRR}/qualimap_out'),
        fastqcReport = '{DATADIR}/{SRR}/qualimap_out/qualimapReport.html'
    params:
        # GENES_GTF = config['GENES_GTF'] #TODO Pull from downloaded gget files
    threads:
        1
        # config['CORES_LO']
    shell:
        """
        mkdir -p {output.qualimapDir}
        cd {output.qualimapDir}

        qualimap rnaseq \
        -bam {input.SORTEDBAM} \
        -gtf {params.GENES_GTF} \
        --sequencing-protocol strand-specific-forward \
        --sorted \
        --java-mem-size=6G \
        -DATADIR {output.qualimapDir} \
        -outformat html
        """

#############################################
## Unmapped read analyses
#############################################

# Compress and run fastqc on unmapped reads; switch names because of STAR weirdness
rule unmapped_fastqc:
    input:
        UNMAPPED1 = '{DATADIR}/{SRR}/Unmapped.out.mate1',
        UNMAPPED2 = '{DATADIR}/{SRR}/Unmapped.out.mate2'
    output:
        UNMAPPED1_FQ = '{DATADIR}/{SRR}/Unmapped.out.mate1.fastq.gz',
        UNMAPPED2_FQ = '{DATADIR}/{SRR}/Unmapped.out.mate2.fastq.gz',
        FQC_DIR = directory('{DATADIR}/{SRR}/Unmapped_fastqc_out')
    params:
        FASTQC_EXEC = config['FASTQC_EXEC']
    threads:
        config['CORES_LO']
    shell:
        """
        mv {input.UNMAPPED1} {input.UNMAPPED2}.fastq
        mv {input.UNMAPPED2} {input.UNMAPPED1}.fastq

        pigz -p{threads} {input.UNMAPPED1}.fastq {input.UNMAPPED2}.fastq

        mkdir -p {output.FQC_DIR}

        {params.FASTQC_EXEC} -o {output.FQC_DIR} -t {threads} {output.UNMAPPED1_FQ} {output.UNMAPPED2_FQ}
        """
