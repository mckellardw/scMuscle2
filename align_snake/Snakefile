########################################################################################################
# align_snake
#   Snakemake workflow to  download fastq files and associated metadata from SRA, then use STARsolo to align and quantify 10x Chromium datasets, and finally perform simple preprocessing so that data are ready for downstream analysis
#   Written by David W. McKellar
########################################################################################################
import pandas as pd
from itertools import chain
from functools import reduce
from os import path, remove, rename

########################################################################################################
# Config files
########################################################################################################
configfile:"config.yaml"

CHEMISTRY_SHEET = pd.read_csv(config["CHEMISTRY_SHEET"], na_filter=False,index_col=0)#"resources/chemistry_sheet.csv"

########################################################################################################
# Executables
########################################################################################################
FFQ_EXEC = config["FFQ_EXEC"]
PREFETCH_EXEC = config["PREFETCH_EXEC"]
FQD_EXEC = config["FQD_EXEC"]
BAM2FQ_EXEC = config["BAM2FQ_EXEC"]

STAR_EXEC = config["STAR_EXEC"]
GGET_EXEC = config["GGET_EXEC"]
FASTQC_EXEC = config["FASTQC_EXEC"]
# TRIMGALORE_EXEC = config["TRIMGALORE_EXEC"]
CUTADAPT_EXEC = config["CUTADAPT_EXEC"]
SAMTOOLS_EXEC = config["SAMTOOLS_EXEC"]
UMITOOLS_EXEC = config["UMITOOLS_EXEC"]
QUALIMAP_EXEC = config["QUALIMAP_EXEC"]
#TODO- samtools, etc

########################################################################################################
# Directories
########################################################################################################
PRODIR = config["PRODIR"]
METADIR = config["METADIR"]
TMPDIR = config["TMPDIR"] # temporary files
DATADIR = config["DATADIR"] # outputs for STAR alignment, etc
REFDIR = config["REFDIR"] # Output for reference genomes

shell("mkdir -p {PRODIR}")
shell("mkdir -p {TMPDIR}")
shell("mkdir -p {METADIR}")
shell("mkdir -p {DATADIR}")
shell("mkdir -p {REFDIR}")

########################################################################################################
# Metadata
########################################################################################################
META = pd.read_csv(config["SAMPLE_SHEET"], na_filter=False)#sep="\t",encoding = "utf-8"

META = META[list(META["include"])] #remove undesired samples ("include"==False)
META = META[list(META["file.format"]!="ERR")] #remove samples with file format issues (#TODO: deprecated?)

# META = META[list(META["tissue"]=="muscle")]
META = META[[x in ["muscle", "tendon","limb", "ligament", "cartilage", "heart"] for x in META["tissue"]]] # ,
# META = META[list(META["tissue"]!="")]
# META = META[list(META["species"]=="Homo sapiens")]
# META = META[[x in ["Homo sapiens", "Mus musculus"] for x in META["species"]]]

META = META[list(META["GSM.accession"]!="NA")]
META = META[list(META["GSM.accession"]!="")]

META = META[[x in ["3p_v2", "3p_v3", "3p_v3.1", "5p_v1"] for x in META["chemistry"]]]

# META = META.iloc[1:3,:] # subset to only download certain SRRs...

########################################################################################################
# Pre-snake metadata prep
########################################################################################################
# Get list of SRR IDs for fastq downloading
SRR_LIST = list(META["SRR.accession"])
for i in range(0,len(SRR_LIST)):
    SRR_LIST[i] = SRR_LIST[i].split(";")
SRR_LIST = list(chain(*SRR_LIST))

# Grab SRR IDs and their associated fastqs
SAMPLES = META["GSM.accession"].unique()

# Build dictionary of SRR IDs/fastq files
R1_FQs = {}
R2_FQs = {}
for GSM in SAMPLES:
    TMP_SRR = META["SRR.accession"][list(META["GSM.accession"]==GSM)].values[0]
    TMP_SRR = TMP_SRR.split(";")
    R1_FQs[GSM] = [DATADIR+"/fastqs/"+ID+"_R1.fastq.gz" for ID in TMP_SRR]
    R2_FQs[GSM] = [DATADIR+"/fastqs/"+ID+"_R2.fastq.gz" for ID in TMP_SRR]

# Build dictionaries of chemistries & species to use for alignment
CHEM_DICT = {}
SPECIES_DICT = {}
for i in range(0,META.shape[0]):
    tmp_gsm = list(META["GSM.accession"])[i]
    CHEM_DICT[tmp_gsm] = list(META["chemistry"])[i]

    SPECIES_DICT[tmp_gsm] = list(META["species"])[i]
    SPECIES_DICT[tmp_gsm] = SPECIES_DICT[tmp_gsm].lower().replace(" ", "_")

########################################################################################################
# Reference genome info
########################################################################################################
SPECIES = list(META["species"]) #all lowercase and underscores (no spaces!)
SPECIES = [x.lower() for x in SPECIES]
SPECIES = [x.replace(" ", "_") for x in SPECIES]
SPECIES = pd.unique(SPECIES)

########################################################################################################
# Snakemake
########################################################################################################
rule all:
    input:
        # expand("{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.dedup.out.bam.bai", DATADIR=config["DATADIR"], sample=SAMPLES), # umi_tools deduplicated .bam **Note** this is super slow!! Only uncomment if NEEDED
        expand("{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate2.fastq.gz", DATADIR=config["DATADIR"], sample=SAMPLES), # compress unmapped reads
        expand("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene/filtered/matrix.mtx.gz", DATADIR=config["DATADIR"], sample=SAMPLES), # compressed count matrices

        # expand("{DATADIR}/align_out/{sample}/STARsolo/Aligned.sortedByCoord.out.bam.bai", DATADIR=config["DATADIR"], sample=SAMPLES), # index non-dedup .bam
        # expand("{DATADIR}/align_out/{sample}/qualimap_out/qualimapReport.html", DATADIR=config["DATADIR"], sample=SAMPLES), # alignment QC with qualimap)

        # expand("{DATADIR}/align_out/{sample}/Unmapped_fastqc_out", DATADIR=config["DATADIR"], sample=SAMPLES), #fastQC results for unmapped reads
        # expand("{DATADIR}/align_out/{sample}/Solo.out/Gene/filtered/matrix.mtx.gz", DATADIR=config["DATADIR"], sample=SAMPLES), # count mats

        expand("{DATADIR}/align_out/{sample}/preTrim_fastqc_R2_out", DATADIR=config["DATADIR"], sample=SAMPLES), # raw R2 fastQC results
        expand("{DATADIR}/align_out/{sample}/postTrim_fastqc_R2_out", DATADIR=config["DATADIR"], sample=SAMPLES), # adapter/polyA/ployG-trimmed R2 fastQC results

        # expand("{TMPDIR}/{SRR}.txt", TMPDIR=config["TMPDIR"], SRR=SRR), # **Only use for debugging**
        expand("{REFDIR}/{SPECIES}/STAR/SAindex", REFDIR=REFDIR, SPECIES=SPECIES), # Reference genomes
        expand("{METADIR}/{SRR}.json", METADIR=config["METADIR"], SRR=SRR_LIST), # metadata files
        expand("{METADIR}/merged_metadata.csv", METADIR=config["METADIR"])

        # expand("{DATADIR}/fastqs/{SRR}_{READ}.fastq.gz", DATADIR=config["DATADIR"], SRR=SRR_LIST, READ=["R1", "R2"]) # fastqs

#############################################
## Get metadata with `ffq`
#############################################
# Download metadata for each individual SRR (run) ID
## Info: https://github.com/pachterlab/ffq
rule get_metadata:
    output:
        METAJSON = "{METADIR}/{SRR}.json"
    shell:
        """
        {FFQ_EXEC} -o {output.METAJSON} {wildcards.SRR}
        """
        # sleep 3

# Combine ffq-downloaded .json"s into a big csv for parsing
#TODO- fix this...
rule merge_metadata:
    input:
        expand("{METADIR}/{SRR}.json", METADIR=config["METADIR"], SRR=SRR_LIST)
    output:
        METACSV = "{METADIR}/merged_metadata.csv"
    run:
        df_list = list()
        for f in input: # load metadata files for each SRR
            with open(f) as data_file:
                data = json.load(data_file)
            df = pd.json_normalize(data).T
            tmp = df.index[0].split(".")[0] + "." # Get SRR ID...
            df = df.rename(index = lambda x: x.strip(tmp)) # Remove SRR ID from row names
            df_list.append(df.T) # transpose so that each row is a different run

        print("Concatenating metadata...")
        df_out = pd.concat(df_list)

        df_out.to_csv(output.METACSV, index=False)

#############################################
## prefetch (SRA-toolkit)
#############################################
# Download reads as an SRA file into a temporary directory, so they can be formatted as .fastq"s
# Data uploaded as a .bam is downloaded in the original format (.bam file) which will later be converted to .fastq
rule prefetch:
    output:
        SRA = temp("{TMPDIR}/{SRR}.sra") # using this b/c of file differences between .fastq and .bam downloading
    params:
        # SRA_tmp = "{TMPDIR}/{SRR}.sra",
        SRA_BAM = "{TMPDIR}/{SRR}.bam",
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in ID for ID in META["SRR.accession"]],"file.format"].values[0]
    run:
        shell(
            f"""
            {PREFETCH_EXEC} \
            --verify yes \
            --type sra \
            --max-size 999G \
            --output-file {output.SRA} \
            {wildcards.SRR}
            """
        )

 # Convert SRA files to fastq with fasterq-dump
rule convert_fastqs:
    input:
        SRA = TMPDIR+"/{SRR}.sra"
    output:
        R1_FQ = temp("{DATADIR}/fastqs/{SRR}_1.fastq.gz"),
        R2_FQ = temp("{DATADIR}/fastqs/{SRR}_2.fastq.gz")
    params:
        # TMPDIR = TMPDIR,
        MEMLIMIT = "4G",
        # SRA_tmp = "{TMPDIR}/{SRR}.sra",
        # SRA_BAM = "{TMPDIR}/{SRR}.bam",
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in ID for ID in META["SRR.accession"]],"file.format"].values[0],
        OUTDIR = "{DATADIR}/fastqs"
    threads:
        config["CORES_LO"]
    run:
        shell(
            f"""
            {FQD_EXEC} \
            --threads {threads} \
            --mem {params.MEMLIMIT} \
            --outdir {params.OUTDIR} \
            --temp {TMPDIR} \
            --split-files \
            --include-technical \
            {input.SRA}

            pigz -p{threads} --force {params.OUTDIR}/{wildcards.SRR}_*.fastq
            """
        )
# && \
# rm {params.SRA_tmp}


# {FFQ} --ncbi {SRR} \
# | xargs curl -o {DATADIR}/fastqs/{SRR}.bam

# Other option:
# ffq --ftp SRR7276476 | grep -Eo '"url": "[^"]*"' | grep -o '"[^"]*"$' | xargs curl -O

# Check fastq sizes, then rename/label files for R1 & R2 (discard I1)
#TODO- account for dual index (v3.1) SRRs!!
#TODO- account for paired-end samples (same R1 and R2 lengths)
rule rename_fastqs:
    input:
        R1_FQ = "{DATADIR}/fastqs/{SRR}_1.fastq.gz",
        R2_FQ = "{DATADIR}/fastqs/{SRR}_2.fastq.gz"
    output:
        R1_FQ = temp("{DATADIR}/fastqs/{SRR}_R1.fastq.gz"),
        R2_FQ = temp("{DATADIR}/fastqs/{SRR}_R2.fastq.gz")
    params:
        OUTDIR = "{DATADIR}/fastqs/",
        UPLOADTYPE = lambda wildcards: \
            META.loc[[wildcards.SRR in ID for ID in META["SRR.accession"]],"file.format"].values[0]
    priority:
        1
    run:
        #TODO- rewrite this into a function that doesn't depend on file size, and can actually do this programmatically...
        # if params.UPLOADTYPE == "fastq":
            # Check for file sizes (R2 is biggest, then R1, then I2 & I1)
            if path.exists(f"{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz"):
                sra_sizes = {
                    f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_4.fastq.gz")
                }
                rename_dict = {
                    "R2": max(sra_sizes),
                    "R1": list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-3])],
                    "I2": list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                    "I1": min(sra_sizes)
                }

                remove(rename_dict["I2"])
                remove(rename_dict["I1"])
                rename(rename_dict["R1"], f"{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz")
                rename(rename_dict["R2"], f"{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz")
            elif path.exists(f"{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz"):
                sra_sizes = {
                    f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_3.fastq.gz")
                }
                rename_dict = {
                    "R2": max(sra_sizes),
                    "R1": list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                    "I1": min(sra_sizes)
                }

                remove(rename_dict["I1"])
                rename(rename_dict["R1"], f"{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz")
                rename(rename_dict["R2"], f"{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz")
            else:
                sra_sizes = {
                    f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_1.fastq.gz"),
                    f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz": path.getsize(f"{DATADIR}/fastqs/{wildcards.SRR}_2.fastq.gz")
                }

                rename_dict = {
                    "R2": max(sra_sizes),
                    "R1": min(sra_sizes)
                }

                rename(rename_dict["R1"], f"{DATADIR}/fastqs/{wildcards.SRR}_R1.fastq.gz")
                rename(rename_dict["R2"], f"{DATADIR}/fastqs/{wildcards.SRR}_R2.fastq.gz")
        # elif params.UPLOADTYPE == "bam":
        #     #rename R1 & R2 files (can use R1 and R2 from file names here, not dependent on file sizes)
        #     shell(
        #         """
        #         mv {input.R1_FQ} {output.R1_FQ}
        #         mv {input.R2_FQ} {output.R2_FQ}
        #         """
        #     )

#############################################
## Alignment prep
#############################################

# Build reference genomes for STARsolo alignment/quantification
#TODO: add check for species files w/ gget; i.e. `gget.ref(species="NA", list_species=True)`
rule build_refs:
    output:
        REF_METADATA = expand("{REFDIR}/{SPECIES}/STAR/metadata.json", REFDIR=config["REFDIR"], SPECIES=SPECIES),
        REF = expand("{REFDIR}/{SPECIES}/STAR/SAindex", REFDIR=config["REFDIR"], SPECIES=SPECIES) # Reference genomes
    threads:
        config["CORES_HI"]
    run:
        for S in SPECIES:
            print(f"Downloading genome sequence and annotations for {S} to {REFDIR}/{S}")
            shell(
                f"""
                mkdir -p {REFDIR}/{S}
                cd {REFDIR}/{S}

                {GGET_EXEC} ref \
                --out {REFDIR}/{S}/metadata.json \
                --which gtf,dna \
                --download \
                {S}

                gunzip {REFDIR}/{S}/*.gz
                """
            )

            # Build reference for STAR
            print(f"Building STAR reference for {S}...\n")
            shell(
                f"""
                {STAR_EXEC} \
                --runThreadN {threads} \
                --runMode genomeGenerate \
                --genomeDir {REFDIR}/{S}/STAR \
                --genomeFastaFiles $(ls -t {REFDIR}/{S}/*.fa) \
                --sjdbGTFfile $(ls -t {REFDIR}/{S}/*.gtf) \
                --sjdbGTFfeatureExon exon

                pigz -p {threads} $(ls -t {REFDIR}/{S}/*.fa)
                pigz -p {threads} $(ls -t {REFDIR}/{S}/*.gtf)
                """
            )

#############################################
## Trimming and FastQC
#############################################
# Merge .fastq files (in case more than one sesquencing run was performed)
#     expand("{DATADIR}/fastqs/{SRR}_{READ}.fastq.gz", DATADIR=config["DATADIR"], SRR=SRR_LIST, READ=["R1", "R2"])
rule merge_fastqs:
    input:
       R1_FQ = lambda wildcards: R1_FQs[wildcards.sample],
       R2_FQ = lambda wildcards: R2_FQs[wildcards.sample]
    output:
        MERGED_R1_FQ = temp("{DATADIR}/align_out/{sample}/tmp/{sample}_R1.fq.gz"),
        MERGED_R2_FQ = temp("{DATADIR}/align_out/{sample}/tmp/{sample}_R2.fq.gz")
    params:
        TMP_DIR = "{DATADIR}/align_out/{sample}/tmp"
# R1_FQ = lambda wildcards: R1_FQs[wildcards.sample],
# R2_FQ = lambda wildcards: R2_FQs[wildcards.sample]
    priority:
        2
    threads:
        config["CORES_LO"]
    run:
        if len(input.R1_FQ)==1 & len(input.R2_FQ)==1: # shell for single fastq input
            shell("cp {input.R1_FQ} {output.MERGED_R1_FQ}")
            shell("cp {input.R2_FQ} {output.MERGED_R2_FQ}")
        else: # shell enablinging multi-fast input; concatenate inputs
            print("Concatenating",len(input.R1_FQ), ".fastq's for", wildcards.sample)
            shell("mkdir -p {params.TMP_DIR}")
            shell("zcat {input.R1_FQ} > {params.TMP_DIR}/{wildcards.sample}_R1.fq")
            shell("zcat {input.R2_FQ} > {params.TMP_DIR}/{wildcards.sample}_R2.fq")
            shell("pigz -p{threads} {params.TMP_DIR}/{wildcards.sample}_R1.fq {params.TMP_DIR}/{wildcards.sample}_R2.fq")

# Check QC of reads before trimming
rule preTrim_FastQC_R2:
    input:
        MERGED_R2_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R2.fq.gz"
    output:
        FASTQC_DIR = directory("{DATADIR}/align_out/{sample}/preTrim_fastqc_R2_out"),
        # fastqcReport = ""
    threads:
        config["CORES_LO"]
        # min([config["CORES_LO"],8]) # 8 core max based on recommendations from trim_galore authors
    run:
        shell(
            f"""
            mkdir -p {output.FASTQC_DIR}
            cd {output.FASTQC_DIR}

            {FASTQC_EXEC} \
            --outdir {output.FASTQC_DIR} \
            --threads {threads} \
            {input.MERGED_R2_FQ}
            """
        )

# TSO, polyA, and polyG trimming
rule cutadapt_R2:
    input:
        MERGED_R1_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R1.fq.gz",
        MERGED_R2_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R2.fq.gz"
    output:
        FINAL_R1_FQ = temp("{DATADIR}/align_out/{sample}/tmp/{sample}_R1_final.fq.gz"),
        FINAL_R2_FQ = temp("{DATADIR}/align_out/{sample}/tmp/{sample}_R2_final.fq.gz")
    params:
        CUTADAPT_EXEC = CUTADAPT_EXEC,
        THREE_PRIME_R2_POLYA = "A"*100, # 100 A-mer
        THREE_PRIME_R2_POLYG = "G"*100, # 100 G-mer
        FIVE_PRIME_R2_TSO = "CCCATGTACTCTGCGTTGATACCACTGCTT", #10x TSO sequence
        FIVE_PRIME_R2_rcTSO = "AAGCAGTGGTATCAACGCAGAGTACATGGG" # rev-comp of 10x TSO sequence
        # FIVE_PRIME_R2 = "TTCGTCACCATAGTTGCGTCTCATGTACCC" #rev 10x TSO sequence
    threads:
        config["CORES_LO"]
        # min([config["CORES_LO"],8]) # 8 core max based on recommendations from trim_galore authors
    log:
        "{DATADIR}/align_out/{sample}/cutadapt.log"
    shell:
        """
        {params.CUTADAPT_EXEC} \
        --minimum-length 18 \
        -A {params.THREE_PRIME_R2_POLYA} \
        -A {params.THREE_PRIME_R2_POLYG} \
 		-G {params.FIVE_PRIME_R2_TSO} \
 		-G {params.FIVE_PRIME_R2_rcTSO} \
        --pair-filter=any \
 		-o {output.FINAL_R1_FQ} \
        -p {output.FINAL_R2_FQ} \
        --cores {threads} \
        {input.MERGED_R1_FQ} {input.MERGED_R2_FQ} 1> {log}
        """

# QC after read trimming
rule postTrim_FastQC_R2:
    input:
        FINAL_R2_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R2_final.fq.gz"
    output:
        FASTQC_DIR = directory("{DATADIR}/align_out/{sample}/postTrim_fastqc_R2_out")
        # fastqcReport = ""
    threads:
        min([config["CORES_LO"],8]) # 8 core max
    run:
        shell(
            f"""
            mkdir -p {output.FASTQC_DIR}
            cd {output.FASTQC_DIR}

            {FASTQC_EXEC} \
            --outdir {output.FASTQC_DIR} \
            --threads {threads} \
            {input.FINAL_R2_FQ}
            """
        )

#############################################
## Alignment
#############################################
# Make output directory, align fastqs, and generate raw/filtered feature/cell-barcode matrices
#   Info for STARsolo command line paramaters: https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md
#  *Note:*
#       - `--soloBarcodeReadLength 0` is included to account for files w/ R1 length > than CB+UMI
#       - `priority` should be set higher to ensure
rule STARsolo_align:
    input:
        FINAL_R1_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R1_final.fq.gz",
        FINAL_R2_FQ = "{DATADIR}/align_out/{sample}/tmp/{sample}_R2_final.fq.gz",
        REF_LIST = expand("{REFDIR}/{SPECIES}/STAR/SAindex", REFDIR=REFDIR, SPECIES=SPECIES) # Reference genomes
    output:
        SORTEDBAM = "{DATADIR}/align_out/{sample}/STARsolo/Aligned.sortedByCoord.out.bam", #TODO: add temp()
        UNMAPPED1 = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate1",
        UNMAPPED2 = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate2",
        VELDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Velocyto"),
        GENEDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene"),
        GENEFULLDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/GeneFull"),
        VELMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Velocyto/filtered/spliced.mtx",
        GENEMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene/filtered/matrix.mtx",
        GENEFULLMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/GeneFull/filtered/matrix.mtx"
    params:
        DATADIR = config["DATADIR"],
        STAR_EXEC = config["STAR_EXEC"],
        MEMLIMIT = config["MEMLIMIT"]
    threads:
        config["CORES_MID"]
    priority:
        42
    run:
        # print(CHEM_DICT)
        tmp_chemistry = CHEM_DICT[wildcards.sample]
        species = SPECIES_DICT[wildcards.sample]

        STAR_REF = f"{REFDIR}/{species}/STAR"
        UMIlen = CHEMISTRY_SHEET["STAR.UMIlen"][tmp_chemistry]
        SOLOtype = CHEMISTRY_SHEET["STAR.soloType"][tmp_chemistry]
        CB_WHITELIST = CHEMISTRY_SHEET["whitelist"][tmp_chemistry]

        # tmp_MEMLIMIT = threads*int(params.MEMLIMIT) # tmp_MEMLIMIT/1000000000
        print("Using up to " + str(params.MEMLIMIT) + " of memory...")
        shell(
            f"""
            mkdir -p {params.DATADIR}/align_out/{wildcards.sample}/STARsolo

            {params.STAR_EXEC} \
            --runThreadN {threads} \
            --outFileNamePrefix {params.DATADIR}/align_out/{wildcards.sample}/STARsolo/ \
            --outSAMtype BAM SortedByCoordinate \
            --outSAMattributes NH HI nM AS CR UR CB UB GX GN sS sQ sM \
            --readFilesCommand zcat \
            --genomeDir {STAR_REF} \
            --limitBAMsortRAM={params.MEMLIMIT} \
            --readFilesIn {input.FINAL_R2_FQ} {input.FINAL_R1_FQ} \
            --clipAdapterType CellRanger4 \
            --outReadsUnmapped Fastx \
            --outFilterMultimapNmax 50 \
            --soloUMIlen {UMIlen} \
            --soloType {SOLOtype} \
            --soloCBwhitelist {CB_WHITELIST} \
            --soloBarcodeReadLength 0 \
            --soloCellFilter EmptyDrops_CR \
            --soloFeatures Gene GeneFull Velocyto \
            --soloMultiMappers EM
            """
        )
        # --genomeLoad LoadAndRemove \

# compress outputs from STAR (count matrices)
rule compress_STAR_outs:
    input:
        VELMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Velocyto/filtered/spliced.mtx",
        GENEMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene/filtered/matrix.mtx",
        GENEFULLMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/GeneFull/filtered/matrix.mtx"
    output:
        VELMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Velocyto/filtered/spliced.mtx.gz",
        GENEMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene/filtered/matrix.mtx.gz",
        GENEFULLMAT = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/GeneFull/filtered/matrix.mtx.gz"
    params:
        # SOLODIR = "{DATADIR}/align_out/{sample}/STARsolo/Solo.out/"
        VELDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Velocyto"),
        GENEDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/Gene"),
        GENEFULLDIR = directory("{DATADIR}/align_out/{sample}/STARsolo/Solo.out/GeneFull")
    threads:
        1
        # config["CORES_LO"]
    run:
        shell(
            f"""
            gzip -qf {params.VELDIR}/*/*.tsv {params.VELDIR}/*/*.mtx
            gzip -qf {params.GENEDIR}/*/*.tsv {params.GENEDIR}/*/*.mtx
            gzip -qf {params.GENEFULLDIR}/*/*.tsv {params.GENEFULLDIR}/*/*.mtx
            """
        )

#TODO convert `mtx` to `h5`
#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html


# Index the .bam produced by STAR
rule indexSortedBAM:
    input:
        SORTEDBAM = "{DATADIR}/align_out/{sample}/STARsolo/Aligned.sortedByCoord.out.bam"
    output:
        BAI = "{DATADIR}/align_out/{sample}/STARsolo/Aligned.sortedByCoord.out.bam.bai"
    params:
        SAMTOOLS_EXEC = SAMTOOLS_EXEC
    threads:
        config["CORES_LO"]
    shell:
        """
        {params.SAMTOOLS_EXEC} index -@ {threads} {input.SORTEDBAM}
        """

#############################################
## Ambient RNA decontamination
#############################################
#TODO

#############################################
## UMI-aware .bam deduplication
#############################################
# Remove reads that don't have a corrected spot/cell barcode with samtools, then remove duplicates w/ **umi-tools**
## High mem usage? Check here! https://umi-tools.readthedocs.io/en/latest/faq.html
## **WARNING** this step is suuuupppppeeerrrr sloowwww. Don't run it if you don't need to!
rule umitools_dedupBAM:
    input:
        CB_WHITELIST = CHEMISTRY_SHEET, #TODO: fix this...
        SORTEDBAM = "{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.out.bam"
    output:
        DEDUPBAM = "{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.dedup.out.bam",
        TMPBAM = temp("{DATADIR}/align_out/{sample}/tmp.bam")
    params:
        SAMTOOLS_EXEC = SAMTOOLS_EXEC,
        UMITOOLS_EXEC = UMITOOLS_EXEC,
        OUTPUT_PREFIX="{DATADIR}/align_out/{sample}/umitools_dedup/{sample}"
        # TMPBAM = "{DATADIR}/align_out/{sample}/tmp.bam"
    threads:
        config["CORES_LO"]
        #1
    log:
        "{DATADIR}/align_out/{sample}/umitools_dedup/dedup.log"
    run:
        shell(
            f"""
            {params.SAMTOOLS_EXEC} view -1 -b \
            -@ {threads} \
            --tag-file CB:{input.CB_WHITELIST} \
            {input.SORTEDBAM} \
            > {output.TMPBAM}

            {params.SAMTOOLS_EXEC} index \
            -@ {threads} \
            {output.TMPBAM}

            {params.UMITOOLS_EXEC} dedup \
            -I {output.TMPBAM} \
            --extract-umi-method=tag \
            --umi-tag=UB \
            --cell-tag=CB \
            --method=unique \
            --per-cell \
            --unmapped-reads=discard \
            --log {log} \
            -S {output.DEDUPBAM} && rm {output.TMPBAM}
            """
        )
# --output-stats={params.OUTPUT_PREFIX} \

rule umitools_indexDedupBAM:
    input:
        SORTEDBAM = "{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.dedup.out.bam"
    output:
        BAI = "{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.dedup.out.bam.bai"
    params:
        SAMTOOLS_EXEC = SAMTOOLS_EXEC
    threads:
        config["CORES_LO"]
    shell:
        """
        {params.SAMTOOLS_EXEC} index -@ {threads} {input.SORTEDBAM}
        """

#############################################
## QC on STAR outputs
#############################################

## qualimap on aligned reads
rule qualimapQC:
    input:
        SORTEDBAM = "{DATADIR}/align_out/{sample}/Aligned.sortedByCoord.out.bam"
    output:
        qualimapDir = directory("{DATADIR}/align_out/{sample}/qualimap_out"),
        fastqcReport = "{DATADIR}/align_out/{sample}/qualimap_out/qualimapReport.html"
    params:
        QUALIMAP_EXEC = QUALIMAP_EXEC
        # GENES_GTF = config["GENES_GTF"] #TODO Pull from downloaded gget files
    threads:
        1
        # config["CORES_LO"]
    shell:
        """
        mkdir -p {output.qualimapDir}
        cd {output.qualimapDir}

        {params.QUALIMAP_EXEC} rnaseq \
        -bam {input.SORTEDBAM} \
        -gtf {params.GENES_GTF} \
        --sequencing-protocol strand-specific-forward \
        --sorted \
        --java-mem-size=6G \
        -DATADIR {output.qualimapDir} \
        -outformat html
        """

#############################################
## Unmapped read analyses
#############################################

# Compress unmapped reads; switch names because of STAR weirdness
rule unmapped_compress:
    input:
        UNMAPPED1 = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate1",
        UNMAPPED2 = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate2"
    output:
        UNMAPPED1_FQ = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate1.fastq.gz",
        UNMAPPED2_FQ = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate2.fastq.gz"
    threads:
        config["CORES_LO"]
    run:
        shell(
            f"""
            mv {input.UNMAPPED1} {input.UNMAPPED2}.fastq
            mv {input.UNMAPPED2} {input.UNMAPPED1}.fastq

            pigz -p {threads} {input.UNMAPPED1}.fastq {input.UNMAPPED2}.fastq
            """
        )

# Run fastqc on unmapped reads
rule unmapped_fastqc:
    input:
        UNMAPPED1_FQ = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate1.fastq.gz",
        UNMAPPED2_FQ = "{DATADIR}/align_out/{sample}/STARsolo/Unmapped.out.mate2.fastq.gz"
    output:
        FQC_DIR = directory("{DATADIR}/align_out/{sample}/Unmapped_fastqc_out")
    threads:
        config["CORES_LO"]
    run:
        shell(
            f"""
            mkdir -p {output.FQC_DIR}

            {FASTQC_EXEC} \
            -o {output.FQC_DIR} \
            -t {threads} \
            {output.UNMAPPED1_FQ} {output.UNMAPPED2_FQ}
            """
        )
