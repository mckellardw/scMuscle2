########################################################################################################
# dwnldfqs_snake
#   Snakemake workflow to download fastq files and associated metadata from SRA
#   v1.0
#   Written by David McKellar
#   Last edited: --/--/--, DWM
########################################################################################################

import pandas as pd
from itertools import chain
from functools import reduce
from os import path, remove, rename

########################################################################################################
# Config file
########################################################################################################
configfile:'config.yaml'

########################################################################################################
# Directories and locations
########################################################################################################
TMPDIR = config['TMPDIR']
PRODIR = config['PRODIR']
METADIR = config['METADIR']

shell("mkdir -p {TMPDIR}")
shell("mkdir -p {PRODIR}")
shell("mkdir -p {METADIR}")

########################################################################################################
# Variables and references
########################################################################################################
META = pd.read_csv(config['SAMPLE_SHEET'], na_filter=False)

META = META[list(META['include'])] #remove undesired samples ('include'==False)
# META = META.iloc[1:150,:]

# Filter for samples with fastq's available
META = META[list(META['GEO.format']=="fastq")]

SRR = list(META['SRR.accession'])
# print(SRR)
for i in range(0,len(SRR)):
    SRR[i] = SRR[i].split(";")
SRR = list(chain(*SRR))

# print(SRR[range(1,12)])

########################################################################################################
# Executables
########################################################################################################
FFQ = config["FFQ"]
PREFETCH = config["PREFETCH"]
PFD = config["PFD"]

########################################################################################################
########################################################################################################
rule all:
    input:
        expand('{PRODIR}/data/fastqs/{sample}_R1.fastq.gz', PRODIR=config['PRODIR'], sample=SRR), # R1 fastq
        expand('{PRODIR}/data/fastqs/{sample}_R2.fastq.gz', PRODIR=config['PRODIR'], sample=SRR), # R2 fastq
        expand('{METADIR}/{sample}.json', METADIR=config['METADIR'], sample=SRR), # metadata files
        expand('{METADIR}/merged_metadata.csv', METADIR=config['METADIR'])

#############################################
## Get metadata with `ffq`
#############################################
# Download metadata for each individual SRR (run) ID
rule get_metadata:
    output:
        METAJSON = '{METADIR}/{sample}.json'
    shell:
        """
        {FFQ} -o {output.METAJSON} {wildcards.sample}
        """

# Combine ffq-downloaded .json's into a big csv for parsing
rule merge_metadata:
    input:
        expand('{METADIR}/{sample}.json', METADIR=config['METADIR'], sample=SRR)
    output:
        METACSV = '{METADIR}/merged_metadata.csv'
    run:
        df_list = list()
        for f in input: # load metadata files for each sample
            with open(f) as data_file:
                data = json.load(data_file)
            df = pd.json_normalize(data).T
            tmp = df.index[0].split('.')[0] + '.' # Get SRR ID...
            df = df.rename(index = lambda x: x.strip(tmp)) # Remove SRR ID from row names
            df_list.append(df.T) # transpose so that each row is a different run

        print("Concatenating metadata...")
        df_out = pd.concat(df_list)

        df_out.to_csv(output.METACSV, index=False)


#############################################
## prefetch (SRA-toolkit)
#############################################
# Download reads as an SRA file into a temporary directory, so they can be formatted as fastq's
rule prefetch:
    output:
        SRA_tmp = temp("{TMPDIR}/{sample}.sra")
    params:
        TMPDIR=TMPDIR
    shell:
        """
        echo "Prefetching SRA files for {wildcards.sample}"

        {PREFETCH} \
        --verify yes \
        --max-size 999999999999 \
        --output-directory {params.TMPDIR} \
        {wildcards.sample}
        """

#############################################
## parallel-fastq-dump
##      https://github.com/rvalieris/parallel-fastq-dump
#############################################
 # Convert SRA files to fastq with parallel-fastq-dump
 #TODO: handle bam files
rule get_fastqs:
    input:
        SRA_tmp = TMPDIR+"/{sample}.sra"
    output:
        R1_FQ = '{PRODIR}/data/fastqs/{sample}_1.fastq.gz',
        R2_FQ = '{PRODIR}/data/fastqs/{sample}_2.fastq.gz'
    params:
        TMPDIR = TMPDIR,
        OUTDIR = '{PRODIR}/data/fastqs/'
    threads:
        config["THREADS"]
    shell:
        """
        echo "Converting SRA files to .fastq's for {wildcards.sample}"

        {PFD} \
        --sra-id {input.SRA_tmp} \
        --threads {threads} \
        --outdir {params.OUTDIR} \
        --tmpdir {params.TMPDIR} \
        --split-files \
        --gzip
        """

        # if UPLOADTYPE == "fastq":
        #     shell(
        #         """
        #         echo "Converting SRA files to .fastq's for {wildcards.sample}"
        #
        #         {PFD} \
        #         --sra-id {input.SRA_tmp} \
        #         --threads {threads} \
        #         --outdir {params.OUTDIR} \
        #         --tmpdir {params.TMPDIR} \
        #         --split-files \
        #         --gzip
        #         """
        #     )
        # elif UPLOADTYPE == "bam":
        #     shell(
        #         """
        #         wget {sample$WGET_LINK}
        #         """
        #     )

# Check fastq sizes, then rename/labem files for R1 & R2 (discard I1)
#TODO- account for dual index (v3.1) samples!!
rule rename_fastqs:
    input:
        R1_FQ = '{PRODIR}/data/fastqs/{sample}_1.fastq.gz',
        R2_FQ = '{PRODIR}/data/fastqs/{sample}_2.fastq.gz'
    output:
        R1_FQ = '{PRODIR}/data/fastqs/{sample}_R1.fastq.gz',
        R2_FQ = '{PRODIR}/data/fastqs/{sample}_R2.fastq.gz'
    run:
        # Check for file sizes (R2 is biggest, then R1, then I2 & I1)
        if path.exists(f'{PRODIR}/data/fastqs/{wildcards.sample}_4.fastq.gz'):
            sra_sizes = {
                f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_3.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_3.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_4.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_4.fastq.gz')
            }
            rename_dict = {
                'R2': max(sra_sizes),
                'R1': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-3])],
                'I2': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                'I1': min(sra_sizes)
            }

            remove(rename_dict['I2'])
            remove(rename_dict['I1'])
            rename(rename_dict['R1'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R1.fastq.gz')
            rename(rename_dict['R2'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R2.fastq.gz')
        elif path.exists(f'{PRODIR}/data/fastqs/{wildcards.sample}_3.fastq.gz'):
            sra_sizes = {
                f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_3.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_3.fastq.gz')
            }
            rename_dict = {
                'R2': max(sra_sizes),
                'R1': list(sra_sizes.keys())[list(sra_sizes.values()).index(sorted(sra_sizes.values())[-2])],
                'I1': min(sra_sizes)
            }

            remove(rename_dict['I1'])
            rename(rename_dict['R1'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R1.fastq.gz')
            rename(rename_dict['R2'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R2.fastq.gz')
        else:
            sra_sizes = {
                f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_1.fastq.gz'),
                f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz': path.getsize(f'{PRODIR}/data/fastqs/{wildcards.sample}_2.fastq.gz')
            }

            rename_dict = {
                'R2': max(sra_sizes),
                'R1': min(sra_sizes)
            }

            rename(rename_dict['R1'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R1.fastq.gz')
            rename(rename_dict['R2'], f'{PRODIR}/data/fastqs/{wildcards.sample}_R2.fastq.gz')
